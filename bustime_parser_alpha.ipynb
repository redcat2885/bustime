{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import geojson\n",
    "from geojson import Feature, FeatureCollection, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = # папка для выгрузки geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 'https://www.bustime.ru/'\n",
    "URL = 'https://www.bustime.ru/nizhniy-novgorod/transport/2021-01-18/'\n",
    "HEADERS = {'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "           'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, params=''):\n",
    "    req = requests.get(url, headers=HEADERS, params=params)\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = get_html(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Забираем id маршрутов (зачем?)\"\"\" \n",
    "\n",
    "def get_id_list(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    route_list = {}\n",
    "    for option in soup.find_all('option'):\n",
    "        route_list[option['value']]=option.text\n",
    "    id_list = list(route_list.keys())\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = get_id_list(html.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Достаём из html страницы все ссылки \"\"\"\n",
    "\n",
    "hrefs = []\n",
    "soup = BeautifulSoup(html.text)\n",
    "for a in soup.find_all('a'):\n",
    "    hrefs.append(a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Из списка с ссылками достаём только страницы маршрутов \"\"\"\n",
    "\"\"\" Переписать в функциональном виде \"\"\"\n",
    "\n",
    "hrefs_sort = []\n",
    "for i in range(len(hrefs)):\n",
    "    if hrefs[i].find('/nizhniy-novgorod/transport/2021-01-18/') == 0:\n",
    "        hrefs_sort.append(hrefs[i])\n",
    "    else:\n",
    "        continue\n",
    "invalid_indices = []\n",
    "for i in range(len(hrefs_sort)):\n",
    "    if hrefs_sort[i].find('page') >= 0:\n",
    "        invalid_indices.append(i)\n",
    "    else:\n",
    "        continue\n",
    "del hrefs_sort[invalid_indices[0]:invalid_indices[-1]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invalid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Создаём список с ссылками на страницы \"\"\"\n",
    "\n",
    "pg = [*range(1,30)]\n",
    "page_urls = []\n",
    "for i in pg:\n",
    "    page_urls.append(URL + 'page-' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTE_URL = HOST + hrefs_sort[0]\n",
    "route_html = get_html(ROUTE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_html.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(route_html.text, 'html.parser')\n",
    "script = soup.find_all('script')\n",
    "# script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = re.compile(r\"var (.*);\",re.DOTALL)\n",
    "result = re.findall(var,str(script))\n",
    "result_list = str(result).split(';')\n",
    "track_line = result_list[3]\n",
    "\n",
    "temp_new = track_line.split('\\\\n')\n",
    "temp_new = ''.join(temp_new)\n",
    "_replace = {'\\\\':' ',\n",
    "           'var track = ':'',\n",
    "           \"'\":'\"',\n",
    "           'uniqueid':'\"uniqueid\"',\n",
    "           'gosnum':'\"gosnum\"',\n",
    "           'bortnum':'\"bortnum\"',\n",
    "           'timestamp':'\"timestamp\"',\n",
    "           'bus_id':'\"bus_id\"',\n",
    "           'heading':'\"heading\"',\n",
    "           'speed':'\"speed\"',\n",
    "           'lon':'\"lon\"',\n",
    "           'lat':'\"lat\"',\n",
    "           'direction':'\"direction\"',\n",
    "           '[':'',\n",
    "           ']':'',\n",
    "           'parseFloat( ':'',\n",
    "           '.replace( \", \", \". \") )':'',\n",
    "           ' ':'',\n",
    "           '},{':'}, {'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_replace(text, dict_):\n",
    "    for i,j in dict_.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "routes_list = mult_replace(temp_new, _replace).split(', ')\n",
    "routes_list[-1] = routes_list[-1].rstrip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geojsonize(_list, path, uid):\n",
    "    features=[]\n",
    "    for i in range(len(_list)):\n",
    "        temp_json = json.loads(_list[i])\n",
    "        geometry = [float(temp_json[\"lon\"].replace(',','.')),\n",
    "                    float(temp_json[\"lat\"].replace(',','.'))]\n",
    "        props = {\"uid\":temp_json[\"uniqueid\"],\n",
    "                 \"gosnum\":temp_json[\"gosnum\"],\n",
    "                 \"bortnum\":temp_json[\"bortnum\"],\n",
    "                 \"timestamp\":temp_json[\"timestamp\"],\n",
    "                 \"bus_id\":temp_json[\"bus_id\"],\n",
    "                 \"heading\":temp_json[\"heading\"],\n",
    "                 \"speed\":temp_json[\"speed\"],\n",
    "                 \"direction\":temp_json[\"direction\"]}\n",
    "        features.append(Feature(geometry=geojson.Point((float(temp_json[\"lon\"].replace(',','.')), \n",
    "                                                        float(temp_json[\"lat\"].replace(',','.')))),\n",
    "                                properties=props))\n",
    "    feature_collection = FeatureCollection(features)\n",
    "    with open(path+'points_'+uid+'.geojson', 'w',encoding='utf8') as f:\n",
    "        geojson.dump(feature_collection, f,ensure_ascii=False)\n",
    "    print('200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs_ = []\n",
    "for i in range(len(page_urls)):\n",
    "    PAGE_URL = page_urls[i]\n",
    "    html = get_html(PAGE_URL)\n",
    "    soup = BeautifulSoup(html.text)\n",
    "    for a in soup.find_all('a'):\n",
    "        hrefs_.append(a['href'])\n",
    "    print(i, 'done')\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hrefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs_sort_ = []\n",
    "for i in range(len(hrefs_)):\n",
    "    if hrefs_[i].find('/nizhniy-novgorod/transport/2021-01-18/') == 0:\n",
    "        hrefs_sort_.append(hrefs_[i])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# СДЕЛАТЬ КОМБИНАЦИЮ С НИЖНИМ ПРАВИЛОМ ЧЕРЕЗ AND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_indices_ = []\n",
    "for i in range(len(hrefs_sort_)):\n",
    "    if hrefs_sort_[i].find('page') >= 0:\n",
    "        invalid_indices_.append(i)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_hrefs = []\n",
    "for i in range(len(invalid_indices_)):\n",
    "    invalid_hrefs.append(hrefs_sort_[invalid_indices_[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs_sort_npg = set(hrefs_sort_) - set(invalid_hrefs)\n",
    "hrefs_sort_npg = list(hrefs_sort_npg)\n",
    "len(hrefs_sort_npg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "fail = 0\n",
    "\n",
    "for i in range(len(hrefs_sort_npg)):\n",
    "    \n",
    "    # скачиваем html страницы автобуса\n",
    "    route_id = hrefs_sort_npg[i].strip('/nizhniy-novgorod/transport/2021-01-18/')\n",
    "    print(route_id+':')\n",
    "    ROUTE_URL = HOST + hrefs_sort_npg[i]\n",
    "    route_html = get_html(ROUTE_URL)\n",
    "    print('\\thtml автобуса',route_id,'скачан')\n",
    "    \n",
    "    # ищем скрипт на странице\n",
    "    soup = BeautifulSoup(route_html.text, 'html.parser')\n",
    "    script = soup.find_all('script')\n",
    "    print('\\tcкрипт автобуса',route_id,'найден')\n",
    "    \n",
    "    # вытаскиваем блок с json\n",
    "    var = re.compile(r\"var (.*);\",re.DOTALL)\n",
    "    result = re.findall(var,str(script))\n",
    "    result_list = str(result).split(';')\n",
    "    track_line = result_list[3]\n",
    "    temp_new = track_line.split('\\\\n')\n",
    "    temp_new = ''.join(temp_new)\n",
    "    print('\\tjson автобуса',route_id,'скачан')\n",
    "    \n",
    "    # убираем весь мусор\n",
    "    routes_list = mult_replace(temp_new, _replace).split(', ')\n",
    "    routes_list[-1] = routes_list[-1].rstrip(',')\n",
    "    \n",
    "    # сохраняем в geojson\n",
    "    print('\\tМаршрут',route_id+'. Статус:')\n",
    "    cnt += 1\n",
    "    try:\n",
    "        geojsonize(routes_list, export_path, route_id)\n",
    "        print('\\n',str(round(cnt/len(hrefs_sort_npg)*100,1))+'%')\n",
    "        print('\\n')\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print('\\t\\t\\t\\tFail')\n",
    "        fail += 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
